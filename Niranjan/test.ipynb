{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "class PreProcessor:\n",
    "    def __init__(self):\n",
    "        self.unique_count = 1\n",
    "    \n",
    "    def RemoveIrrelevantColumn(self,df):\n",
    "        count=0\n",
    "        for column in df.columns:\n",
    "            if df[column].nunique(dropna=True)==self.unique_count:\n",
    "                print(f'Column :{column} is removed')\n",
    "                count=count+1\n",
    "                df=df.drop(column,axis=1)\n",
    "        print(f'{count} irrelavant columns found.')\n",
    "        return df\n",
    "\n",
    "        \n",
    "\n",
    "    def HandlingMissingData(self,df,num_strategy='most_frequent',cat_strategy='knn',n_neighbors=3,null_threshold=0.1):\n",
    "        num_cols = df.select_dtypes(include=np.number).columns\n",
    "        cat_cols = [column for column in df.columns if column not in num_cols]\n",
    "        \n",
    "        for column in num_cols:\n",
    "            null_val=df[column].isnull().mean()\n",
    "            if(null_val!=0 and null_val<=null_threshold):\n",
    "                print(f'{null_val}% NaN values found on column: {column}')\n",
    "                df=df.dropna(subset=[column])\n",
    "                df= df.reset_index(drop=True)\n",
    "\n",
    "        imputer=None\n",
    "        if(num_strategy=='knn'):\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        else:\n",
    "            if num_strategy in ['mean','median','mode','most_frequent']:\n",
    "                imputer = SimpleImputer(strategy=num_strategy)\n",
    "            else:\n",
    "                print('Invalid imputer strategy specified :{}\\nDefault strategy Mean is applied',num_strategy)\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "        print('imputation process started...')\n",
    "        for feature in num_cols:\n",
    "            if df[feature].isna().sum().sum() != 0:\n",
    "                try:\n",
    "                    df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
    "                    if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                        df[feature] = df_imputed\n",
    "                        # round back to INTs, if original data were INTs\n",
    "                        df[feature] = df[feature].round()\n",
    "                        df[feature] = df[feature].astype('Int64')                                        \n",
    "                    else:\n",
    "                        df[feature] = df_imputed\n",
    "                except:\n",
    "                    print('imputation failed for feature \"{}\"',feature)\n",
    "        if(cat_strategy=='knn'):\n",
    "            imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "        elif(cat_strategy=='logreq'):\n",
    "            df = PreProcessor.LogisticRegressionImputer(\n",
    "                columns=cat_cols,\n",
    "                df=df\n",
    "            )\n",
    "            return df\n",
    "        else:\n",
    "            imputer = SimpleImputer(strategy='most_frequent')\n",
    "        \n",
    "        for feature in cat_cols:\n",
    "            if df[feature].isna().sum()!= 0:\n",
    "                try:\n",
    "                    mapping = dict()\n",
    "                    mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
    "                    mapping[feature] = mappings\n",
    "                    df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "                    df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
    "\n",
    "                    # round to integers before mapping back to original values\n",
    "                    df[feature] = df_imputed\n",
    "                    df[feature] = df[feature].round()\n",
    "                    df[feature] = df[feature].astype('Int64')  \n",
    "\n",
    "                    # map values back to original\n",
    "                    mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                    df[feature] = df[feature].map(mappings_inv)\n",
    "                except:\n",
    "                    print('Imputation failed for feature \"{}\"',  feature)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def normalization(self,df):\n",
    "        sc = StandardScaler()\n",
    "        normalize_columns = []\n",
    "        for column in df.columns:\n",
    "            if (df[column].dtype == 'int64' or df[column].dtype == 'float64') and df[column].nunique() > 10:\n",
    "                normalize_columns.append(column)\n",
    "        df[normalize_columns] = sc.fit_transform(df[normalize_columns])\n",
    "        # Is normalization Done well\n",
    "        normalized_feature = df[normalize_columns]\n",
    "        mean_normalized = np.mean(normalized_feature)\n",
    "        std_dev_normalized = np.std(normalized_feature)\n",
    "        print(\"Mean of normalized feature:\", mean_normalized)\n",
    "        print(\"Standard deviation of normalized feature:\", std_dev_normalized)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def encoding(self,df):   \n",
    "        lable_encoder = preprocessing.LabelEncoder() \n",
    "        for column in df.columns:\n",
    "                if df[column].dtype == 'object' and df[column].nunique()<3: #binary \n",
    "                    df[column]=lable_encoder.fit_transform(df[column])\n",
    "                elif df[column].dtype == 'object' and (df[column].nunique()>2 and df[column].nunique()<=12): #Multi-class \n",
    "                    df[column]=lable_encoder.fit_transform(df[column])\n",
    "                elif df[column].dtype == 'bool':\n",
    "                    df[column]=lable_encoder.fit_transform(df[column])\n",
    "                else:\n",
    "                    pass\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def LogisticRegressionImputer(self,columns,df):\n",
    "             for feature in columns:\n",
    "                 try:\n",
    "                     test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                     train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                     if len(test_df.index) != 0:\n",
    "                         pipe = make_pipeline(StandardScaler(), LogisticRegression())  \n",
    "                         y = train_df[feature]\n",
    "                         train_df.drop(feature, axis=1, inplace=True)\n",
    "                         test_df.drop(feature, axis=1, inplace=True)   \n",
    "                         model = pipe.fit(train_df, y)\n",
    "                       \n",
    "                         pred = model.predict(test_df) # predict values\n",
    "                         test_df[feature]= pred\n",
    "                         if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                             # round back to INTs, if original data were INTs\n",
    "                             test_df[feature] = test_df[feature].round()\n",
    "                             test_df[feature] = test_df[feature].astype('Int64')\n",
    "                             df[feature].update(test_df[feature])                             \n",
    "                         print('LOGREG imputation of {} value(s) succeeded for feature \"{}\"', len(pred), feature)\n",
    "                 except:\n",
    "                     print('LOGREG imputation failed for feature \"{}\"', feature)\n",
    "                 for feature in df.columns: \n",
    "                     try:\n",
    "                         # map categorical feature values back to original\n",
    "                         mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                         df[feature] = df[feature].map(mappings_inv)\n",
    "                     except:\n",
    "                         pass     \n",
    "             return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column :Site is removed\n",
      "Column :Provisional or Ratified is removed\n",
      "2 irrelavant columns found.\n",
      "0.024315068493150686% NaN values found on column: Value\n",
      "imputation process started...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>ReadingDateTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2022 00:15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2022 00:30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2022 01:15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>01/01/2022 01:30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102559</th>\n",
       "      <td>2</td>\n",
       "      <td>31/12/2022 22:45</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102560</th>\n",
       "      <td>2</td>\n",
       "      <td>31/12/2022 23:00</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102561</th>\n",
       "      <td>2</td>\n",
       "      <td>31/12/2022 23:15</td>\n",
       "      <td>9.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102562</th>\n",
       "      <td>2</td>\n",
       "      <td>31/12/2022 23:30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102563</th>\n",
       "      <td>2</td>\n",
       "      <td>31/12/2022 23:45</td>\n",
       "      <td>8.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102564 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Species   ReadingDateTime  Value  Units\n",
       "0             0  01/01/2022 00:00    0.7      0\n",
       "1             0  01/01/2022 00:15    0.5      0\n",
       "2             0  01/01/2022 00:30    0.3      0\n",
       "3             0  01/01/2022 01:15    0.6      0\n",
       "4             0  01/01/2022 01:30    0.5      0\n",
       "...         ...               ...    ...    ...\n",
       "102559        2  31/12/2022 22:45   12.2      1\n",
       "102560        2  31/12/2022 23:00    9.9      1\n",
       "102561        2  31/12/2022 23:15    9.6      1\n",
       "102562        2  31/12/2022 23:30    8.0      1\n",
       "102563        2  31/12/2022 23:45    8.5      1\n",
       "\n",
       "[102564 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./LaqnData.csv')\n",
    "clean = PreProcessor()\n",
    "data = clean.RemoveIrrelevantColumn(data)\n",
    "data = clean.HandlingMissingData(data)\n",
    "# data= clean.normalization(data)\n",
    "data=clean.encoding(data)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 102564 entries, 0 to 102563\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count   Dtype  \n",
      "---  ------           --------------   -----  \n",
      " 0   Species          102564 non-null  int32  \n",
      " 1   ReadingDateTime  102564 non-null  object \n",
      " 2   Value            102564 non-null  float64\n",
      " 3   Units            102564 non-null  int32  \n",
      "dtypes: float64(1), int32(2), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
