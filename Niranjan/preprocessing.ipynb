{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from yellowbrick import FeatureImportances\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "# from feature_engine import missing_data_imputers as mdi\n",
    "# from feature_engine import discretisers as dsc\n",
    "# from feature_engine import encoding as ce\n",
    "# from feature_engine import transformations as vt\n",
    "import optuna\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 105120 entries, 0 to 105119\n",
      "Data columns (total 6 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   Site                     105120 non-null  object \n",
      " 1   Species                  105120 non-null  object \n",
      " 2   ReadingDateTime          105120 non-null  object \n",
      " 3   Value                    102564 non-null  float64\n",
      " 4   Units                    105120 non-null  object \n",
      " 5   Provisional or Ratified  105120 non-null  object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 4.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  Site Species   ReadingDateTime  Value   Units Provisional or Ratified\n",
       " 0  EN5      NO  01/01/2022 00:00    0.7  ug m-3                       R\n",
       " 1  EN5      NO  01/01/2022 00:15    0.5  ug m-3                       R\n",
       " 2  EN5      NO  01/01/2022 00:30    0.3  ug m-3                       R\n",
       " 3  EN5      NO  01/01/2022 00:45    NaN  ug m-3                       R\n",
       " 4  EN5      NO  01/01/2022 01:00    NaN  ug m-3                       R,\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./LaqnData.csv')\n",
    "df.head(),df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling irrelavant column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>ReadingDateTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 01:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105115</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 22:45</td>\n",
       "      <td>12.2</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105116</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:00</td>\n",
       "      <td>9.9</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105117</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:15</td>\n",
       "      <td>9.6</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105118</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105119</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:45</td>\n",
       "      <td>8.5</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105120 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Species   ReadingDateTime  Value          Units\n",
       "0           NO  01/01/2022 00:00    0.7         ug m-3\n",
       "1           NO  01/01/2022 00:15    0.5         ug m-3\n",
       "2           NO  01/01/2022 00:30    0.3         ug m-3\n",
       "3           NO  01/01/2022 00:45    NaN         ug m-3\n",
       "4           NO  01/01/2022 01:00    NaN         ug m-3\n",
       "...        ...               ...    ...            ...\n",
       "105115     NOX  31/12/2022 22:45   12.2  ug m-3 as NO2\n",
       "105116     NOX  31/12/2022 23:00    9.9  ug m-3 as NO2\n",
       "105117     NOX  31/12/2022 23:15    9.6  ug m-3 as NO2\n",
       "105118     NOX  31/12/2022 23:30    8.0  ug m-3 as NO2\n",
       "105119     NOX  31/12/2022 23:45    8.5  ug m-3 as NO2\n",
       "\n",
       "[105120 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for column in df.columns:\n",
    "    if df[column].nunique(dropna=True)==1:\n",
    "        df=df.drop(column,axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>ReadingDateTime</th>\n",
       "      <th>Value</th>\n",
       "      <th>Units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:00</td>\n",
       "      <td>0.7</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 00:30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 01:15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO</td>\n",
       "      <td>01/01/2022 01:30</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ug m-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102559</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 22:45</td>\n",
       "      <td>12.2</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102560</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:00</td>\n",
       "      <td>9.9</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102561</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:15</td>\n",
       "      <td>9.6</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102562</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:30</td>\n",
       "      <td>8.0</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102563</th>\n",
       "      <td>NOX</td>\n",
       "      <td>31/12/2022 23:45</td>\n",
       "      <td>8.5</td>\n",
       "      <td>ug m-3 as NO2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102564 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Species   ReadingDateTime  Value          Units\n",
       "0           NO  01/01/2022 00:00    0.7         ug m-3\n",
       "1           NO  01/01/2022 00:15    0.5         ug m-3\n",
       "2           NO  01/01/2022 00:30    0.3         ug m-3\n",
       "3           NO  01/01/2022 01:15    0.6         ug m-3\n",
       "4           NO  01/01/2022 01:30    0.5         ug m-3\n",
       "...        ...               ...    ...            ...\n",
       "102559     NOX  31/12/2022 22:45   12.2  ug m-3 as NO2\n",
       "102560     NOX  31/12/2022 23:00    9.9  ug m-3 as NO2\n",
       "102561     NOX  31/12/2022 23:15    9.6  ug m-3 as NO2\n",
       "102562     NOX  31/12/2022 23:30    8.0  ug m-3 as NO2\n",
       "102563     NOX  31/12/2022 23:45    8.5  ug m-3 as NO2\n",
       "\n",
       "[102564 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "null_matrix =[]\n",
    "null_threshold = 0.05\n",
    "\n",
    "for column in df.columns:\n",
    "    null_matrix.append(df[column].isnull().mean())\n",
    "# print(null_matrix)\n",
    "for i in range(0,len(null_matrix)):\n",
    "    if np.issubdtype(df[df.columns[i]].dtype, np.character) or (null_matrix[i]<=null_threshold and null_matrix[i]!=0.0):\n",
    "        df=df.dropna(subset=[df.columns[i]])\n",
    "        df= df.reset_index(drop=True)\n",
    "    elif null_matrix[i]>null_threshold:\n",
    "        df[df.columns[i]] = df[df.columns[i]].fillna(df[df.columns[i]].mean())\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoClean 2022\n",
    "# For detailed documentation and usage guide, please visit the official GitHub Repo.\n",
    "# https://github.com/elisemercury/AutoClean\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from loguru import logger\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "'''\n",
    "Modules are used by the AutoClean pipeline for data cleaning and preprocessing.\n",
    "'''\n",
    "\n",
    "class MissingValues:\n",
    "\n",
    "    def handle(self, df, _n_neighbors=3):\n",
    "        # function for handling missing values in the data\n",
    "        if self.missing_num or self.missing_categ:\n",
    "            logger.info('Started handling of missing values...', str(self.missing_num).upper())\n",
    "            start = timer()\n",
    "            self.count_missing = df.isna().sum().sum()\n",
    "\n",
    "            if self.count_missing != 0:\n",
    "                logger.info('Found a total of {} missing value(s)', self.count_missing)\n",
    "                df = df.dropna(how='all')\n",
    "                df.reset_index(drop=True)\n",
    "                \n",
    "                if self.missing_num: # numeric data\n",
    "                    logger.info('Started handling of NUMERICAL missing values... Method: \"{}\"', str(self.missing_num).upper())\n",
    "                    # automated handling\n",
    "                    if self.missing_num == 'auto': \n",
    "                        self.missing_num = 'linreg'\n",
    "                        lr = LinearRegression()\n",
    "                        df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "                        self.missing_num = 'knn'\n",
    "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='num')\n",
    "                    # linear regression imputation\n",
    "                    elif self.missing_num == 'linreg':\n",
    "                        lr = LinearRegression()\n",
    "                        df = MissingValues._lin_regression_impute(self, df, lr)\n",
    "                    # knn imputation\n",
    "                    elif self.missing_num == 'knn':\n",
    "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='num')\n",
    "                    # mean, median or mode imputation\n",
    "                    elif self.missing_num in ['mean', 'median', 'most_frequent']:\n",
    "                        imputer = SimpleImputer(strategy=self.missing_num)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='num')\n",
    "                    # delete missing values\n",
    "                    elif self.missing_num == 'delete':\n",
    "                        df = MissingValues._delete(self, df, type='num')\n",
    "                        logger.debug('Deletion of {} NUMERIC missing value(s) succeeded', self.count_missing-df.isna().sum().sum())      \n",
    "\n",
    "                if self.missing_categ: # categorical data\n",
    "                    logger.info('Started handling of CATEGORICAL missing values... Method: \"{}\"', str(self.missing_categ).upper())\n",
    "                    # automated handling\n",
    "                    if self.missing_categ == 'auto':\n",
    "                        self.missing_categ = 'logreg'\n",
    "                        lr = LogisticRegression()\n",
    "                        df = MissingValues._log_regression_impute(self, df, lr)\n",
    "                        self.missing_categ = 'knn'\n",
    "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "                    elif self.missing_categ == 'logreg':\n",
    "                        lr = LogisticRegression()\n",
    "                        df = MissingValues._log_regression_impute(self, df, lr)\n",
    "                    # knn imputation\n",
    "                    elif self.missing_categ == 'knn':\n",
    "                        imputer = KNNImputer(n_neighbors=_n_neighbors)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='categ')  \n",
    "                    # mode imputation\n",
    "                    elif self.missing_categ == 'most_frequent':\n",
    "                        imputer = SimpleImputer(strategy=self.missing_categ)\n",
    "                        df = MissingValues._impute(self, df, imputer, type='categ')\n",
    "                    # delete missing values                    \n",
    "                    elif self.missing_categ == 'delete':\n",
    "                        df = MissingValues._delete(self, df, type='categ')\n",
    "                        logger.debug('Deletion of {} CATEGORICAL missing value(s) succeeded', self.count_missing-df.isna().sum().sum())\n",
    "            else:\n",
    "                logger.debug('{} missing values found', self.count_missing)\n",
    "            end = timer()\n",
    "            logger.info('Completed handling of missing values in {} seconds', round(end-start, 6))  \n",
    "        else:\n",
    "            logger.info('Skipped handling of missing values')\n",
    "        return df\n",
    "\n",
    "    def _impute(self, df, imputer, type):\n",
    "        # function for imputing missing values in the data\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "\n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    if df[feature].isna().sum().sum() != 0:\n",
    "                        try:\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)))\n",
    "                            counter = df[feature].isna().sum().sum() - df_imputed.isna().sum().sum()\n",
    "\n",
    "                            if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                                df[feature] = df_imputed\n",
    "                                # round back to INTs, if original data were INTs\n",
    "                                df[feature] = df[feature].round()\n",
    "                                df[feature] = df[feature].astype('Int64')                                        \n",
    "                            else:\n",
    "                                df[feature] = df_imputed\n",
    "                            if counter != 0:\n",
    "                                logger.debug('{} imputation of {} value(s) succeeded for feature \"{}\"', str(self.missing_num).upper(), counter, feature)\n",
    "                        except:\n",
    "                            logger.warning('{} imputation failed for feature \"{}\"', str(self.missing_num).upper(), feature)\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    if df[feature].isna().sum()!= 0:\n",
    "                        try:\n",
    "                            mapping = dict()\n",
    "                            mappings = {k: i for i, k in enumerate(df[feature].dropna().unique(), 0)}\n",
    "                            mapping[feature] = mappings\n",
    "                            df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "                            df_imputed = pd.DataFrame(imputer.fit_transform(np.array(df[feature]).reshape(-1, 1)), columns=[feature])    \n",
    "                            counter = sum(1 for i, j in zip(list(df_imputed[feature]), list(df[feature])) if i != j)\n",
    "\n",
    "                            # round to integers before mapping back to original values\n",
    "                            df[feature] = df_imputed\n",
    "                            df[feature] = df[feature].round()\n",
    "                            df[feature] = df[feature].astype('Int64')  \n",
    "\n",
    "                            # map values back to original\n",
    "                            mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                            df[feature] = df[feature].map(mappings_inv)\n",
    "                            if counter != 0:\n",
    "                                logger.debug('{} imputation of {} value(s) succeeded for feature \"{}\"', self.missing_categ.upper(), counter, feature)\n",
    "                        except:\n",
    "                            logger.warning('{} imputation failed for feature \"{}\"', str(self.missing_categ).upper(), feature)\n",
    "        return df\n",
    "\n",
    "    def _lin_regression_impute(self, df, model):\n",
    "        # function for predicting missing values with linear regression\n",
    "        cols_num = df.select_dtypes(include=np.number).columns\n",
    "        mapping = dict()\n",
    "        for feature in df.columns:\n",
    "            if feature not in cols_num:\n",
    "                # create label mapping for categorical feature values\n",
    "                mappings = {k: i for i, k in enumerate(df[feature])}\n",
    "                mapping[feature] = mappings\n",
    "                df[feature] = df[feature].map(mapping[feature])\n",
    "        for feature in cols_num: \n",
    "                try:\n",
    "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    if len(test_df.index) != 0:\n",
    "                        pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "                        y = np.log(train_df[feature]) # log-transform the data\n",
    "                        X_train = train_df.drop(feature, axis=1)\n",
    "                        test_df.drop(feature, axis=1, inplace=True)\n",
    "                        \n",
    "                        try:\n",
    "                            model = pipe.fit(X_train, y)\n",
    "                        except:\n",
    "                            y = train_df[feature] # use non-log-transformed data\n",
    "                            model = pipe.fit(X_train, y)\n",
    "                        if (y == train_df[feature]).all():\n",
    "                            pred = model.predict(test_df)\n",
    "                        else:\n",
    "                            pred = np.exp(model.predict(test_df)) # predict values\n",
    "\n",
    "                        test_df[feature]= pred\n",
    "\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            # round back to INTs, if original data were INTs\n",
    "                            test_df[feature] = test_df[feature].round()\n",
    "                            test_df[feature] = test_df[feature].astype('Int64')\n",
    "                            df[feature].update(test_df[feature])                          \n",
    "                        else:\n",
    "                            df[feature].update(test_df[feature])  \n",
    "                        logger.debug('LINREG imputation of {} value(s) succeeded for feature \"{}\"', len(pred), feature)\n",
    "                except:\n",
    "                    logger.warning('LINREG imputation failed for feature \"{}\"', feature)\n",
    "        for feature in df.columns: \n",
    "            try:   \n",
    "                # map categorical feature values back to original\n",
    "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                df[feature] = df[feature].map(mappings_inv)\n",
    "            except:\n",
    "                pass\n",
    "        return df\n",
    "\n",
    "    def _log_regression_impute(self, df, model):\n",
    "        # function for predicting missing values with logistic regression\n",
    "        cols_num = df.select_dtypes(include=np.number).columns\n",
    "        mapping = dict()\n",
    "        for feature in df.columns:\n",
    "            if feature not in cols_num:\n",
    "                # create label mapping for categorical feature values\n",
    "                mappings = {k: i for i, k in enumerate(df[feature])} #.dropna().unique(), 0)}\n",
    "                mapping[feature] = mappings\n",
    "                df[feature] = df[feature].map(mapping[feature])\n",
    "\n",
    "        target_cols = [x for x in df.columns if x not in cols_num]\n",
    "            \n",
    "        for feature in df.columns: \n",
    "            if feature in target_cols:\n",
    "                try:\n",
    "                    test_df = df[df[feature].isnull()==True].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    train_df = df[df[feature].isnull()==False].dropna(subset=[x for x in df.columns if x != feature])\n",
    "                    if len(test_df.index) != 0:\n",
    "                        pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "                        y = train_df[feature]\n",
    "                        train_df.drop(feature, axis=1, inplace=True)\n",
    "                        test_df.drop(feature, axis=1, inplace=True)\n",
    "\n",
    "                        model = pipe.fit(train_df, y)\n",
    "                        \n",
    "                        pred = model.predict(test_df) # predict values\n",
    "                        test_df[feature]= pred\n",
    "\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            # round back to INTs, if original data were INTs\n",
    "                            test_df[feature] = test_df[feature].round()\n",
    "                            test_df[feature] = test_df[feature].astype('Int64')\n",
    "                            df[feature].update(test_df[feature])                             \n",
    "                        logger.debug('LOGREG imputation of {} value(s) succeeded for feature \"{}\"', len(pred), feature)\n",
    "                except:\n",
    "                    logger.warning('LOGREG imputation failed for feature \"{}\"', feature)\n",
    "        for feature in df.columns: \n",
    "            try:\n",
    "                # map categorical feature values back to original\n",
    "                mappings_inv = {v: k for k, v in mapping[feature].items()}\n",
    "                df[feature] = df[feature].map(mappings_inv)\n",
    "            except:\n",
    "                pass     \n",
    "        return df\n",
    "\n",
    "    def _delete(self, df, type):\n",
    "        # function for deleting missing values\n",
    "        cols_num = df.select_dtypes(include=np.number).columns \n",
    "        if type == 'num':\n",
    "            # numerical features\n",
    "            for feature in df.columns: \n",
    "                if feature in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        else:\n",
    "            # categorical features\n",
    "            for feature in df.columns:\n",
    "                if feature not in cols_num:\n",
    "                    df = df.dropna(subset=[feature])\n",
    "                    df.reset_index(drop=True)\n",
    "        return df                    \n",
    "\n",
    "class Outliers:\n",
    "\n",
    "    def handle(self, df):\n",
    "        # function for handling of outliers in the data\n",
    "        if self.outliers:\n",
    "            logger.info('Started handling of outliers... Method: \"{}\"', str(self.outliers).upper())\n",
    "            start = timer()  \n",
    "\n",
    "            if self.outliers in ['auto', 'winz']:  \n",
    "                df = Outliers._winsorization(self, df)\n",
    "            elif self.outliers == 'delete':\n",
    "                df = Outliers._delete(self, df)\n",
    "            \n",
    "            end = timer()\n",
    "            logger.info('Completed handling of outliers in {} seconds', round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped handling of outliers')\n",
    "        return df     \n",
    "\n",
    "    def _winsorization(self, df):\n",
    "        # function for outlier winsorization\n",
    "        cols_num = df.select_dtypes(include=np.number).columns    \n",
    "        for feature in cols_num:           \n",
    "            counter = 0\n",
    "            # compute outlier bounds\n",
    "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
    "            for row_index, row_val in enumerate(df[feature]):\n",
    "                if row_val < lower_bound or row_val > upper_bound:\n",
    "                    if row_val < lower_bound:\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                                df.loc[row_index, feature] = lower_bound\n",
    "                                df[feature] = df[feature].astype(int) \n",
    "                        else:    \n",
    "                            df.loc[row_index, feature] = lower_bound\n",
    "                        counter += 1\n",
    "                    else:\n",
    "                        if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                            df.loc[row_index, feature] = upper_bound\n",
    "                            df[feature] = df[feature].astype(int) \n",
    "                        else:\n",
    "                            df.loc[row_index, feature] = upper_bound\n",
    "                        counter += 1\n",
    "            if counter != 0:\n",
    "                logger.debug('Outlier imputation of {} value(s) succeeded for feature \"{}\"', counter, feature)        \n",
    "        return df\n",
    "\n",
    "    def _delete(self, df):\n",
    "        # function for deleting outliers in the data\n",
    "        cols_num = df.select_dtypes(include=np.number).columns    \n",
    "        for feature in cols_num:\n",
    "            counter = 0\n",
    "            lower_bound, upper_bound = Outliers._compute_bounds(self, df, feature)    \n",
    "            # delete observations containing outliers            \n",
    "            for row_index, row_val in enumerate(df[feature]):\n",
    "                if row_val < lower_bound or row_val > upper_bound:\n",
    "                    df = df.drop(row_index)\n",
    "                    counter +=1\n",
    "            df = df.reset_index(drop=True)\n",
    "            if counter != 0:\n",
    "                logger.debug('Deletion of {} outliers succeeded for feature \"{}\"', counter, feature)\n",
    "        return df\n",
    "\n",
    "    def _compute_bounds(self, df, feature):\n",
    "        # function that computes the lower and upper bounds for finding outliers in the data\n",
    "        featureSorted = sorted(df[feature])\n",
    "        \n",
    "        q1, q3 = np.percentile(featureSorted, [25, 75])\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        lb = q1 - (self.outlier_param * iqr) \n",
    "        ub = q3 + (self.outlier_param * iqr) \n",
    "\n",
    "        return lb, ub    \n",
    "\n",
    "class Adjust:\n",
    "\n",
    "    def convert_datetime(self, df):\n",
    "        # function for extracting of datetime values in the data\n",
    "        if self.extract_datetime:\n",
    "            logger.info('Started conversion of DATETIME features... Granularity: {}', self.extract_datetime)\n",
    "            start = timer()\n",
    "            cols = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
    "            for feature in cols: \n",
    "                try:\n",
    "                    # convert features encoded as strings to type datetime ['D','M','Y','h','m','s']\n",
    "                    df[feature] = pd.to_datetime(df[feature], infer_datetime_format=True)\n",
    "                    try:\n",
    "                        df['Day'] = pd.to_datetime(df[feature]).dt.day\n",
    "\n",
    "                        if self.extract_datetime in ['auto', 'M','Y','h','m','s']:\n",
    "                            df['Month'] = pd.to_datetime(df[feature]).dt.month\n",
    "\n",
    "                            if self.extract_datetime in ['auto', 'Y','h','m','s']:\n",
    "                                df['Year'] = pd.to_datetime(df[feature]).dt.year\n",
    "\n",
    "                                if self.extract_datetime in ['auto', 'h','m','s']:\n",
    "                                    df['Hour'] = pd.to_datetime(df[feature]).dt.hour\n",
    "\n",
    "                                    if self.extract_datetime in ['auto', 'm','s']:\n",
    "                                        df['Minute'] = pd.to_datetime(df[feature]).dt.minute\n",
    "\n",
    "                                        if self.extract_datetime in ['auto', 's']:\n",
    "                                            df['Sec'] = pd.to_datetime(df[feature]).dt.second\n",
    "                        \n",
    "                        logger.debug('Conversion to DATETIME succeeded for feature \"{}\"', feature)\n",
    "\n",
    "                        try: \n",
    "                            # check if entries for the extracted dates/times are non-NULL, otherwise drop\n",
    "                            if (df['Hour'] == 0).all() and (df['Minute'] == 0).all() and (df['Sec'] == 0).all():\n",
    "                                df.drop('Hour', inplace = True, axis =1 )\n",
    "                                df.drop('Minute', inplace = True, axis =1 )\n",
    "                                df.drop('Sec', inplace = True, axis =1 )\n",
    "                            elif (df['Day'] == 0).all() and (df['Month'] == 0).all() and (df['Year'] == 0).all():\n",
    "                                df.drop('Day', inplace = True, axis =1 )\n",
    "                                df.drop('Month', inplace = True, axis =1 )\n",
    "                                df.drop('Year', inplace = True, axis =1 )   \n",
    "                        except:\n",
    "                            pass          \n",
    "                    except:\n",
    "                        # feature cannot be converted to datetime\n",
    "                        logger.warning('Conversion to DATETIME failed for \"{}\"', feature)\n",
    "                except:\n",
    "                    pass\n",
    "            end = timer()\n",
    "            logger.info('Completed conversion of DATETIME features in {} seconds', round(end-start, 4))\n",
    "        else:\n",
    "            logger.info('Skipped datetime feature conversion')\n",
    "        return df\n",
    "\n",
    "    def round_values(self, df, input_data):\n",
    "        # function that checks datatypes of features and converts them if necessary\n",
    "        if self.duplicates or self.missing_num or self.missing_categ or self.outliers or self.encode_categ or self.extract_datetime:\n",
    "            logger.info('Started feature type conversion...')\n",
    "            start = timer()\n",
    "            counter = 0\n",
    "            cols_num = df.select_dtypes(include=np.number).columns\n",
    "            for feature in cols_num:\n",
    "                    # check if all values are integers\n",
    "                    if (df[feature].fillna(-9999) % 1  == 0).all():\n",
    "                        try:\n",
    "                            # encode FLOATs with only 0 as decimals to INT\n",
    "                            df[feature] = df[feature].astype('Int64')\n",
    "                            counter += 1\n",
    "                            logger.debug('Conversion to type INT succeeded for feature \"{}\"', feature)\n",
    "                        except:\n",
    "                            logger.warning('Conversion to type INT failed for feature \"{}\"', feature)\n",
    "                    else:\n",
    "                        try:\n",
    "                            df[feature] = df[feature].astype(float)\n",
    "                            # round the number of decimals of FLOATs back to original\n",
    "                            dec = None\n",
    "                            for value in input_data[feature]:\n",
    "                                try:\n",
    "                                    if dec == None:\n",
    "                                        dec = str(value)[::-1].find('.')\n",
    "                                    else:\n",
    "                                        if str(value)[::-1].find('.') > dec:\n",
    "                                            dec = str(value)[::-1].find('.')\n",
    "                                except:\n",
    "                                    pass\n",
    "                            df[feature] = df[feature].round(decimals = dec)\n",
    "                            counter += 1\n",
    "                            logger.debug('Conversion to type FLOAT succeeded for feature \"{}\"', feature)\n",
    "                        except:\n",
    "                            logger.warning('Conversion to type FLOAT failed for feature \"{}\"', feature)\n",
    "            end = timer()\n",
    "            logger.info('Completed feature type conversion for {} feature(s) in {} seconds', counter, round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped feature type conversion')\n",
    "        return df\n",
    "\n",
    "class EncodeCateg:\n",
    "\n",
    "    def handle(self, df):\n",
    "        # function for encoding of categorical features in the data\n",
    "        if self.encode_categ:\n",
    "            if not isinstance(self.encode_categ, list):\n",
    "                self.encode_categ = ['auto']\n",
    "            # select non numeric features\n",
    "            cols_categ = set(df.columns) ^ set(df.select_dtypes(include=np.number).columns) \n",
    "            # check if all columns should be encoded\n",
    "            if len(self.encode_categ) == 1:\n",
    "                target_cols = cols_categ # encode ALL columns\n",
    "            else:\n",
    "                target_cols = self.encode_categ[1] # encode only specific columns\n",
    "            logger.info('Started encoding categorical features... Method: \"{}\"', str(self.encode_categ[0]).upper())\n",
    "            start = timer()\n",
    "            for feature in target_cols:\n",
    "                if feature in cols_categ:\n",
    "                    # columns are column names\n",
    "                    feature = feature\n",
    "                else:\n",
    "                    # columns are indexes\n",
    "                    feature = df.columns[feature]\n",
    "                try:\n",
    "                    # skip encoding of datetime features\n",
    "                    pd.to_datetime(df[feature])\n",
    "                    logger.debug('Skipped encoding for DATETIME feature \"{}\"', feature)\n",
    "                except:\n",
    "                    try:\n",
    "                        if self.encode_categ[0] == 'auto':\n",
    "                            # ONEHOT encode if not more than 10 unique values to encode\n",
    "                            if df[feature].nunique() <=10:\n",
    "                                df = EncodeCateg._to_onehot(self, df, feature)\n",
    "                                logger.debug('Encoding to ONEHOT succeeded for feature \"{}\"', feature)\n",
    "                            # LABEL encode if not more than 20 unique values to encode\n",
    "                            elif df[feature].nunique() <=20:\n",
    "                                df = EncodeCateg._to_label(self, df, feature)\n",
    "                                logger.debug('Encoding to LABEL succeeded for feature \"{}\"', feature)\n",
    "                            # skip encoding if more than 20 unique values to encode\n",
    "                            else:\n",
    "                                logger.debug('Encoding skipped for feature \"{}\"', feature)   \n",
    "\n",
    "                        elif self.encode_categ[0] == 'onehot':\n",
    "                            df = EncodeCateg._to_onehot(df, feature)\n",
    "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)\n",
    "                        elif self.encode_categ[0] == 'label':\n",
    "                            df = EncodeCateg._to_label(df, feature)\n",
    "                            logger.debug('Encoding to {} succeeded for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)      \n",
    "                    except:\n",
    "                        logger.warning('Encoding to {} failed for feature \"{}\"', str(self.encode_categ[0]).upper(), feature)    \n",
    "            end = timer()\n",
    "            logger.info('Completed encoding of categorical features in {} seconds', round(end-start, 6))\n",
    "        else:\n",
    "            logger.info('Skipped encoding of categorical features')\n",
    "        return df\n",
    "\n",
    "    def _to_onehot(self, df, feature, limit=10):  \n",
    "        # function that encodes categorical features to OneHot encodings    \n",
    "        one_hot = pd.get_dummies(df[feature], prefix=feature)\n",
    "        if one_hot.shape[1] > limit:\n",
    "            logger.warning('ONEHOT encoding for feature \"{}\" creates {} new features. Consider LABEL encoding instead.', feature, one_hot.shape[1])\n",
    "        # join the encoded df\n",
    "        df = df.join(one_hot)\n",
    "        return df\n",
    "\n",
    "    def _to_label(self, df, feature):\n",
    "        # function that encodes categorical features to label encodings \n",
    "        le = preprocessing.LabelEncoder()\n",
    "\n",
    "        df[feature + '_lab'] = le.fit_transform(df[feature].values)\n",
    "        mapping = dict(zip(le.classes_, range(len(le.classes_))))\n",
    "        \n",
    "        for key in mapping:\n",
    "            try:\n",
    "                if isnan(key):               \n",
    "                    replace = {mapping[key] : key }\n",
    "                    df[feature].replace(replace, inplace=True)\n",
    "            except:\n",
    "                pass\n",
    "        return df  \n",
    "\n",
    "class Duplicates:\n",
    "\n",
    "    def handle(self, df):\n",
    "        if self.duplicates:\n",
    "            logger.info('Started handling of duplicates... Method: \"{}\"', str(self.duplicates).upper())\n",
    "            start = timer()\n",
    "            original = df.shape\n",
    "            try:\n",
    "                df.drop_duplicates(inplace=True, ignore_index=False)\n",
    "                df = df.reset_index(drop=True)\n",
    "                new = df.shape\n",
    "                count = original[0] - new[0]\n",
    "                if count != 0:\n",
    "                    logger.debug('Deletion of {} duplicate(s) succeeded', count)\n",
    "                else:\n",
    "                    logger.debug('{} missing values found', count)\n",
    "                end = timer()\n",
    "                logger.info('Completed handling of duplicates in {} seconds', round(end-start, 6))\n",
    "\n",
    "            except:\n",
    "                logger.warning('Handling of duplicates failed')        \n",
    "        else:\n",
    "            logger.info('Skipped handling of duplicates')\n",
    "        return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting python-telegram-bot\n",
      "  Obtaining dependency information for python-telegram-bot from https://files.pythonhosted.org/packages/e7/69/285c31caff09a10ce932711a63835775ed7c503783bd808a837ce803f055/python_telegram_bot-20.7-py3-none-any.whl.metadata\n",
      "  Downloading python_telegram_bot-20.7-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting schedule\n",
      "  Obtaining dependency information for schedule from https://files.pythonhosted.org/packages/f0/1a/e5a65c08d708ee4c002f3f938ba69b2282af478755700c40f037eacc92ef/schedule-1.2.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading schedule-1.2.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting httpx~=0.25.2 (from python-telegram-bot)\n",
      "  Obtaining dependency information for httpx~=0.25.2 from https://files.pythonhosted.org/packages/a2/65/6940eeb21dcb2953778a6895281c179efd9100463ff08cb6232bb6480da7/httpx-0.25.2-py3-none-any.whl.metadata\n",
      "  Downloading httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.25.2->python-telegram-bot) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.25.2->python-telegram-bot) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx~=0.25.2->python-telegram-bot)\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.25.2->python-telegram-bot) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx~=0.25.2->python-telegram-bot) (1.2.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx~=0.25.2->python-telegram-bot)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/58.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading python_telegram_bot-20.7-py3-none-any.whl (552 kB)\n",
      "   ---------------------------------------- 0.0/552.6 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 143.4/552.6 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 399.4/552.6 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 552.6/552.6 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading schedule-1.2.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/75.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 75.0/75.0 kB ? eta 0:00:00\n",
      "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.9/76.9 kB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: schedule, h11, httpcore, httpx, python-telegram-bot\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 python-telegram-bot-20.7 schedule-1.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\asus\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install python-telegram-bot schedule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6485297009:AAH8Iu0IwwMSbWVnTF1WpIhZVKEKqgEkAp8\n",
    "import time\n",
    "from telegram import Bot\n",
    "from telegram.error import TelegramError\n",
    "import schedule\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Replace 'YOUR_API_KEY' with your actual Telegram API key\n",
    "# API_KEY = '6485297009:AAH8Iu0IwwMSbWVnTF1WpIhZVKEKqgEkAp8'\n",
    "# TARGET_USERNAME = 'eashu301003'\n",
    "\n",
    "# bot = Bot(token=API_KEY)\n",
    "\n",
    "# async def check_online():\n",
    "#     try:\n",
    "#         user = await bot.get_chat(TARGET_USERNAME)\n",
    "#         print(user)\n",
    "#         if user.status == 'online':\n",
    "#             print(f\"{TARGET_USERNAME} is online!\")\n",
    "#             # You can add your notification logic here (e.g., send a message or trigger another notification)\n",
    "#         else:\n",
    "#             print(f\"{TARGET_USERNAME} is offline.\")\n",
    "#     except TelegramError as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "# check_online()\n",
    "# # # Check online status every 5 minutes\n",
    "# # schedule.every(1).minutes.do(check_online)\n",
    "\n",
    "# # while True:\n",
    "# #     schedule.run_pending()\n",
    "# #     time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Chat not found\n"
     ]
    }
   ],
   "source": [
    "API_KEY = '6485297009:AAH8Iu0IwwMSbWVnTF1WpIhZVKEKqgEkAp8'\n",
    "TARGET_USERNAME = '@eashu301003'\n",
    "async def check_online():\n",
    "    bot = Bot(token=API_KEY)\n",
    "    try:\n",
    "        user = await bot.get_chat(TARGET_USERNAME)\n",
    "        print(user)\n",
    "        if user.status == 'online':\n",
    "            print(f\"{TARGET_USERNAME} is online!\")\n",
    "            # You can add your notification logic here (e.g., send a message or trigger another notification)\n",
    "        else:\n",
    "            print(f\"{TARGET_USERNAME} is offline.\")\n",
    "    except TelegramError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "async def main():\n",
    "    while True:\n",
    "        await check_online()\n",
    "        await asyncio.sleep(300)  # Sleep for 300 seconds (adjust as needed)\n",
    "\n",
    "# Use the existing event loop in Jupyter\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "except RuntimeError:\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "loop.run_until_complete(await main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
